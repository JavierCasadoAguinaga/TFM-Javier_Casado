{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javier/anaconda3/lib/python3.7/site-packages/IPython/core/magics/pylab.py:160: UserWarning:\n",
      "\n",
      "pylab import has clobbered these variables: ['datetime']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import plotly.graph_objects as go\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from glob import glob\n",
    "import re\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXTRAER LOS NOMBRES DE TODOS LOS ARCHIVOS PARA SU POSTERIOR LECTURA Y ORDENARLOS EN ORDEN CRONOLÓGICO\n",
    "archivos = glob('/home/javier/REPOS-GH/TFM-Javier_Casado/DATOS/*Semana*.txt')\n",
    "\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "\n",
    "    return [ atoi(c) for c in re.split(r'(\\d+)', text) ]\n",
    "\n",
    "archivos.sort(key=natural_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREAMOS LOS CSVs VACÍOS PARA ALMACENAR LOS DFs DE CADENA PARA SARIMAX\n",
    "data_prueba = pd.DataFrame(columns=['Target'])\n",
    "data_prueba.to_csv('/home/javier/REPOS-GH/TFM-Javier_Casado/SARIMAX/atres.csv')\n",
    "data_prueba.to_csv('/home/javier/REPOS-GH/TFM-Javier_Casado/SARIMAX/sexta.csv')\n",
    "data_prueba.to_csv('/home/javier/REPOS-GH/TFM-Javier_Casado/SARIMAX/neox.csv')\n",
    "data_prueba.to_csv('/home/javier/REPOS-GH/TFM-Javier_Casado/SARIMAX/nova.csv')\n",
    "data_prueba.to_csv('/home/javier/REPOS-GH/TFM-Javier_Casado/SARIMAX/mega.csv')\n",
    "data_prueba.to_csv('/home/javier/REPOS-GH/TFM-Javier_Casado/SARIMAX/atreseries.csv')\n",
    "data_prueba.to_csv('/home/javier/REPOS-GH/TFM-Javier_Casado/SARIMAX/telecinco.csv')\n",
    "data_prueba.to_csv('/home/javier/REPOS-GH/TFM-Javier_Casado/SARIMAX/cuatro.csv')\n",
    "data_prueba.to_csv('/home/javier/REPOS-GH/TFM-Javier_Casado/SARIMAX/divinity.csv')\n",
    "data_prueba.to_csv('/home/javier/REPOS-GH/TFM-Javier_Casado/SARIMAX/energy.csv')\n",
    "data_prueba.to_csv('/home/javier/REPOS-GH/TFM-Javier_Casado/SARIMAX/fdf.csv')\n",
    "data_prueba.to_csv('/home/javier/REPOS-GH/TFM-Javier_Casado/SARIMAX/bemad.csv')\n",
    "data_prueba.to_csv('/home/javier/REPOS-GH/TFM-Javier_Casado/SARIMAX/dmax.csv')\n",
    "data_prueba.to_csv('/home/javier/REPOS-GH/TFM-Javier_Casado/SARIMAX/dkiss.csv')\n",
    "data_prueba.to_csv('/home/javier/REPOS-GH/TFM-Javier_Casado/SARIMAX/paramount.csv')\n",
    "data_prueba.to_csv('/home/javier/REPOS-GH/TFM-Javier_Casado/SARIMAX/ten.csv')\n",
    "data_prueba.to_csv('/home/javier/REPOS-GH/TFM-Javier_Casado/SARIMAX/trece.csv')\n",
    "\n",
    "data_ml = pd.DataFrame(columns=['Full_date','Time','Date', 'Day_name', 'Month_name','Year','Time_frame'\n",
    "                                ,'Channel','Sales_house','Title/Descrip', 'Genre','Public','Production_comp'\n",
    "                                ,'Age_rating','Adults_avg_audience'])\n",
    "data_ml.to_csv('/home/javier/REPOS-GH/TFM-Javier_Casado/ML_ALGOR/df_ml.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCIÓN PARA CREAR LA COLUMNA DE FRANJAS HORARIAS A PARTIR DE VALOR DE FECHA.\n",
    "def time_frame (value):\n",
    "    \n",
    "    if ((value.time() >= datetime.time(7,0)) & (value < datetime.time(14,0))):\n",
    "        return 'Morning'\n",
    "    if ((value.time() >= datetime.time(14,0)) & (value < datetime.time(16,0))):\n",
    "        return 'Lunch'\n",
    "    if ((value.time() >= datetime.time(16,0)) & (value < datetime.time(20,30))):\n",
    "        return 'After'\n",
    "    if ((value.time() >= datetime.time(20,30)) & (value <= datetime.time(23,59))):\n",
    "        return 'PT'\n",
    "    if ((value.time() >= datetime.time(0,0)) & (value < datetime.time(0,30))):\n",
    "        return 'PT'\n",
    "    if ((value.time() >= datetime.time(0,30)) & (value < datetime.time(1,30))):\n",
    "        return 'Late_PT'\n",
    "    if ((value.time() >= datetime.time(1,30)) & (value < datetime.time(7,0))):\n",
    "        return 'Night'\n",
    "    \n",
    "\n",
    "#FUNCIÓN PARA CREAR EL GRUPO COMERCIAL DE CADA CADENA\n",
    "def com_group(cadena):\n",
    "    atres = ['A3','LA SEXTA','NEOX','NOVA','MEGA','ATRESERIES']\n",
    "    medset = ['T5','CUATRO','DIVINITY','ENERGY','FDF','BEMADtv']\n",
    "    pulsa = ['DMAX','DKISS','PARAMOUNT NETWORK','TEN','TRECE']\n",
    "    \n",
    "    if cadena in atres:\n",
    "        return 'ATRESMEDIA'\n",
    "    if cadena in medset:\n",
    "        return 'MEDIASET'\n",
    "    if cadena in pulsa:\n",
    "        return 'PULSA'\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/javier/REPOS-GH/TFM-Javier_Casado/DATOS/2018 - Semana 1 (07012018).txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javier/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:90: FutureWarning:\n",
      "\n",
      "Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proc_ok\n",
      "ok\n",
      "/home/javier/REPOS-GH/TFM-Javier_Casado/DATOS/2018 - Semana 2 (14012018).txt\n",
      "proc_ok\n",
      "ok\n",
      "/home/javier/REPOS-GH/TFM-Javier_Casado/DATOS/2018 - Semana 3 (21012018).txt\n",
      "proc_ok\n",
      "ok\n",
      "/home/javier/REPOS-GH/TFM-Javier_Casado/DATOS/2018 - Semana 4 (28012018).txt\n",
      "proc_ok\n",
      "ok\n",
      "/home/javier/REPOS-GH/TFM-Javier_Casado/DATOS/2018 - Semana 5 (04022018).txt\n",
      "proc_ok\n",
      "ok\n",
      "/home/javier/REPOS-GH/TFM-Javier_Casado/DATOS/2018 - Semana 6 (11022018).txt\n",
      "proc_ok\n",
      "ok\n",
      "/home/javier/REPOS-GH/TFM-Javier_Casado/DATOS/2018 - Semana 7 (18022018).txt\n",
      "proc_ok\n",
      "ok\n",
      "/home/javier/REPOS-GH/TFM-Javier_Casado/DATOS/2018 - Semana 8 (25022018).txt\n",
      "proc_ok\n",
      "ok\n",
      "/home/javier/REPOS-GH/TFM-Javier_Casado/DATOS/2018 - Semana 9 (04032018).txt\n",
      "proc_ok\n",
      "ok\n",
      "/home/javier/REPOS-GH/TFM-Javier_Casado/DATOS/2018 - Semana 10 (11032018).txt\n",
      "proc_ok\n",
      "ok\n",
      "/home/javier/REPOS-GH/TFM-Javier_Casado/DATOS/2018 - Semana 11 (18032018).txt\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "counter = 0\n",
    "for arch in archivos: \n",
    "    print(arch)\n",
    "    #pd.set_option('display.max_columns', None)\n",
    "    #pd.set_option('display.max_rows', None)\n",
    "    df = pd.read_csv(arch,error_bad_lines=False,encoding = 'unicode_escape', \n",
    "                     header=3, sep='\\t', skiprows=[4], skipinitialspace=True, \n",
    "                     usecols=['Título','Título/Descripción', 'Cadena', 'Fecha', 'Género','Público objetivo', \n",
    "                              'Productora','Calificación de Edad','Ind. 16+ (c/inv.)'],dtype ='str' )\n",
    "\n",
    "    #ELIMINAMOS LAS CABECERAS DE LOS PROGRAMAS EN LA COLUMNA TÍTULO\n",
    "    df.dropna(inplace = True)\n",
    "    df = df[df.Título.str.startswith('<<')]\n",
    "    df.reset_index(drop = True,inplace = True)\n",
    "\n",
    "    #ELIMINAMOS LOS SIMBOLOS <> DEL DATA FRAME\n",
    "    df1 = pd.DataFrame()\n",
    "    for i in df.columns:\n",
    "        df1 = pd.concat([df1,df.loc[:,i].str.translate({ord(x): None for x in ('><')})], axis = 1)\n",
    "\n",
    "    #ESCOJO LAS CADENAS DE INTERÉS\n",
    "    cadenas_v = ['A3','LA SEXTA','NEOX','NOVA','MEGA','ATRESERIES','T5','CUATRO','DIVINITY','ENERGY','FDF'\n",
    "                 ,'BEMADtv','DMAX','DKISS','PARAMOUNT NETWORK','TEN','TRECE']\n",
    "    df1 = df1[df1['Cadena'].isin(cadenas_v)]\n",
    "\n",
    "    #DIVIDIMOS LA COLUMNA EN DOS PARA PODER OPERAR SOLO CON LA HORA \n",
    "    #(NO SE PUEDE PASAR A DATETIME YA QUE NO ESTÁ EN EL RANGO ADECUADO)\n",
    "    df1[['Hora','Minuto']] = df1.Título.str.split(':', expand = True)\n",
    "    #df1.head(5)\n",
    "\n",
    "    #PASAMOS LA COLUMNA FECHA A DATETIME PARA PODER RESTAR DIAS CUANDO SEA NECESARIO\n",
    "    df1['Fecha'] = pd.to_datetime(df1['Fecha'],format = \"%d/%m/%Y\", dayfirst=True)\n",
    "    #df1.dtypes\n",
    "\n",
    "    #SE CAMBIA EL TIPO DE DATO EN LAS COLUMNAS HORA MINUTO PARA PODER TRANSFORMAR A RANGO 00:00-23:00(STR-INT)\n",
    "    df1 = df1.astype({'Hora':'int','Minuto':'int'})\n",
    "    #df1.dtypes\n",
    "\n",
    "    #SUMAMOS 1 DÍA A LAS HORAS SUPERIORES A 23\n",
    "    ind = df1[df1['Hora']>23].index\n",
    "    df1.loc[ind,'Fecha']= df1.loc[ind,'Fecha'] + timedelta(days=1)\n",
    "\n",
    "    #CONVERTIMOS PARA TENER RANGO 00-23\n",
    "    df1.Hora = df1.Hora.replace({24:0,25:1,26:2})\n",
    "\n",
    "    #SE SUSTITUYE EL 24 POR 0 PARA TENER RANGO 00-23\n",
    "    #df1.Hora = df1.Hora.replace(24,0)\n",
    "\n",
    "\n",
    "    #CONVIERTO LA FECHA A STRING PARA PODER UNIRLO CON LA HORA\n",
    "    df1['Fecha']=df1.Fecha.apply(lambda x:str(x.date()))\n",
    "    df1[df1['Cadena']=='La1'].head(1413)\n",
    "\n",
    "    #VOLVEMOS A CONVERTIR HORA Y MINUTO A STR PARA PODER UNIRLOS CON ':' COMO SEPARADOR\n",
    "    df1 = df1.astype({'Hora':'str', 'Minuto':'str'})\n",
    "\n",
    "    #UNIMOS HORA Y MINUTO PARA DESPUÉS UNIRLO A LA FECHA Y CONVERTIRLO EN TIMESTAMP\n",
    "    df1 ['Tiempo'] = df1[['Hora', 'Minuto']].apply(lambda x: ':'.join(x), axis=1)\n",
    "    df1 ['Fecha_comp'] = df1[['Fecha', 'Tiempo']].apply(lambda x: ' '.join(x), axis=1)\n",
    "    df1.Fecha_comp = pd.to_datetime(df1.Fecha_comp)\n",
    "\n",
    "    #ELIMINAMOS LAS COLUMNAS GENERADAS Y USADAS PARA LAS TRANSFORMACIONES\n",
    "    df1 = df1.drop(['Tiempo','Hora', 'Minuto','Fecha','Título'], axis = 1)\n",
    "    #,'Descripción'\n",
    "\n",
    "    #CAMBIO COMA POR PUNTO PARA PODER CONVERTIR A ARRAY DE FLOAT\n",
    "    #column = ['Ind. 4+ (inv)','Amas de Casa (c/inv.)', 'Ind. 4 - 12 (c/inv.)','Ind. 16+ (c/inv.)',\n",
    "     #         'Ind. 4+ (inv).1','Amas de Casa (c/inv.).1','Ind. 4 - 12 (c/inv.).1', 'Ind. 16+ (c/inv.).1']\n",
    "    column = 'Ind. 16+ (c/inv.)'\n",
    "    df1['Ind. 16+ (c/inv.)']=df1['Ind. 16+ (c/inv.)'].str.replace(',','.')\n",
    "   # for i in column:\n",
    "    #    df1[i]=df1[i].str.replace(',','.')\n",
    "\n",
    "    #CREO LA VARIABLE TARGET CON LAS COLUMNAS NECESARIAS\n",
    "    df1['Target'] = df1['Ind. 16+ (c/inv.)'].apply(lambda x : float(x))\n",
    "    #df1['Target']= df1[column].apply(lambda x:'/'.join(x),axis=1)\n",
    "    #df1['Target']= df1.Target.str.split('/').apply(lambda x : [float(i) for i in x])\n",
    "    #df1['Target']= df1.Target.str.split('/').apply(lambda x:np.array(x,dtype=np.float32))\n",
    "\n",
    "    #ELIMINAMOS LAS VARIABLES UTILIZADAS EN TARGET\n",
    "    df1 = df1.drop(column,axis = 1)\n",
    "\n",
    "    #ELIMINAMOS TITULO/DESCRIPCION DE MOMENTO (!)\n",
    "    #df1 = df1.drop('Título/Descripción',axis = 1)\n",
    "\n",
    "    #ORDENO LAS COLUMNAS DE MI DATA FRAME (!) \n",
    "    df1 = df1[['Fecha_comp','Título/Descripción','Cadena', 'Género', 'Público objetivo', 'Productora',\n",
    "           'Calificación de Edad', 'Target']]\n",
    "    \n",
    "    #ELIMINO LOS VALORES DUPLICADOS POR CADENA Y ME QUEDO CON LOS MÁXIMOS\n",
    "    df1 = df1.groupby(['Fecha_comp','Cadena'])['Título/Descripción', 'Género', 'Público objetivo', 'Productora',\n",
    "       'Calificación de Edad','Target'].agg({'Target':'max'}).reset_index(level=[0,1],col_level = 1)\n",
    "    df1.columns = df1.columns.droplevel(level = 0)\n",
    "    \n",
    "    #AÑADIMOS LAS COLUMNAS NECESARIAS PARA REALIZAR LAS AGRUPACIONES DEL FRONTEND\n",
    "    df1['Year'] = df1.Fecha_comp.dt.year\n",
    "    df1['Date'] = df1.Fecha_comp.dt.date\n",
    "    df1['Time'] = df1.Fecha_comp.dt.time\n",
    "    df1['Day_name'] = df1.Fecha_comp.dt.day_name()\n",
    "    df1['Month_name'] = df1.Fecha_comp.dt.month_name()\n",
    "\n",
    "    df1['Sales_house'] = df1.Cadena.swifter.apply(lambda cadena: com_group(cadena))\n",
    "\n",
    "    df1['Time_frame'] = df1.Fecha_comp.swifter.apply(lambda valor:time_frame(valor))\n",
    "    \n",
    "\n",
    "    #SELECCIONAMOS LAS COLUMNAS NECESARIAS PARA MODELO SARIMAX \n",
    "    pr = df1[['Fecha_comp','Cadena','Target']]\n",
    "    \n",
    "    \n",
    "    #TRADUZCO LAS COLUMNAS PARA TRABAJAR EN INGLÉS\n",
    "    df1 = df1.rename(columns={'Fecha_comp':'Full_date', 'Cadena':'Channel','Título/Descripción':'Title/Descrip',\n",
    "                        'Género':'Genre','Público objetivo':'Public','Productora':'Production_comp'\n",
    "                        ,'Calificación de Edad':'Age_rating','Target':'Adults_avg_audience'})\n",
    "    \n",
    "    #ORDENO LAS COLUMNAS COMO ME INTERESA\n",
    "    df1 = df1[['Full_date', 'Time', 'Date', 'Day_name', 'Month_name', 'Year','Time_frame', 'Channel', 'Sales_house'\n",
    "               ,'Title/Descrip', 'Genre','Public', 'Production_comp', 'Age_rating', 'Adults_avg_audience']]\n",
    "    \n",
    "\n",
    "    #SEPARAMOS DATAFRAMES EN CADENAS PARA DESARROLLAR LOS ALGORITMOS PERTINENTES PARA CADA UNO\n",
    "\n",
    "    grupos = pr.groupby('Cadena')\n",
    "\n",
    "    dfatrs = grupos.get_group('A3').groupby('Fecha_comp')['Target'].max().to_frame()\n",
    "    dfsxt = grupos.get_group('LA SEXTA').groupby('Fecha_comp')['Target'].max().to_frame()\n",
    "    dfnx = grupos.get_group('NEOX').groupby('Fecha_comp')['Target'].max().to_frame()\n",
    "    dfnv = grupos.get_group('NOVA').groupby('Fecha_comp')['Target'].max().to_frame()\n",
    "    dfmg = grupos.get_group('MEGA').groupby('Fecha_comp')['Target'].max().to_frame()\n",
    "    dfatrser = grupos.get_group('ATRESERIES').groupby('Fecha_comp')['Target'].max().to_frame()\n",
    "    dftcnc = grupos.get_group('T5').groupby('Fecha_comp')['Target'].max().to_frame()\n",
    "    dfctr = grupos.get_group('CUATRO').groupby('Fecha_comp')['Target'].max().to_frame()\n",
    "    dfdvnt = grupos.get_group('DIVINITY').groupby('Fecha_comp')['Target'].max().to_frame()\n",
    "    dfnrg = grupos.get_group('ENERGY').groupby('Fecha_comp')['Target'].max().to_frame()\n",
    "    dffdf = grupos.get_group('FDF').groupby('Fecha_comp')['Target'].max().to_frame()\n",
    "    dfbmd = grupos.get_group('BEMADtv').groupby('Fecha_comp')['Target'].max().to_frame()\n",
    "    dfdmx = grupos.get_group('DMAX').groupby('Fecha_comp')['Target'].max().to_frame()\n",
    "    dfdks = grupos.get_group('DKISS').groupby('Fecha_comp')['Target'].max().to_frame()\n",
    "    dfprmnt = grupos.get_group('PARAMOUNT NETWORK').groupby('Fecha_comp')['Target'].max().to_frame()\n",
    "    dftn = grupos.get_group('TEN').groupby('Fecha_comp')['Target'].max().to_frame()\n",
    "    dftrc = grupos.get_group('TRECE').groupby('Fecha_comp')['Target'].max().to_frame()\n",
    "    print('proc_ok')\n",
    "    \n",
    "    #GUARDAMOS LOS DF DE CADENA EN ARCHIVOS CSV CREADOS PREVIAMENTE PARA CONCATENAR EL PROCESADO DE LOS DATOS\n",
    "    dfatrs.to_csv('/home/javier/REPOS-GH/TFM-Javier_Casado/SARIMAX/atres.csv', mode = 'a', header = False)\n",
    "    dfsxt.to_csv('/home/javier/REPOS-GH/TFM-Javier_Casado/SARIMAX/sexta.csv', mode = 'a', header = False)\n",
    "    dfnx.to_csv('/home/javier/REPOS-GH/TFM-Javier_Casado/SARIMAX/neox.csv', mode = 'a', header = False)\n",
    "    dfnv.to_csv('/home/javier/REPOS-GH/TFM-Javier_Casado/SARIMAX/nova.csv', mode = 'a', header = False)\n",
    "    dfmg.to_csv('/home/javier/REPOS-GH/TFM-Javier_Casado/SARIMAX/mega.csv', mode = 'a', header = False)\n",
    "    dfatrser.to_csv('/home/javier/REPOS-GH/TFM-Javier_Casado/SARIMAX/atreseries.csv', mode = 'a', header = False)\n",
    "    dftcnc.to_csv('/home/javier/REPOS-GH/TFM-Javier_Casado/SARIMAX/telecinco.csv', mode = 'a', header = False)\n",
    "    dfctr.to_csv('/home/javier/REPOS-GH/TFM-Javier_Casado/SARIMAX/cuatro.csv', mode = 'a', header = False)\n",
    "    dfdvnt.to_csv('/home/javier/REPOS-GH/TFM-Javier_Casado/SARIMAX/divinity.csv', mode = 'a', header = False)\n",
    "    dfnrg.to_csv('/home/javier/REPOS-GH/TFM-Javier_Casado/SARIMAX/energy.csv', mode = 'a', header = False)\n",
    "    dffdf.to_csv('/home/javier/REPOS-GH/TFM-Javier_Casado/SARIMAX/fdf.csv', mode = 'a', header = False)\n",
    "    dfbmd.to_csv('/home/javier/REPOS-GH/TFM-Javier_Casado/SARIMAX/bemad.csv', mode = 'a', header = False)\n",
    "    dfdmx.to_csv('/home/javier/REPOS-GH/TFM-Javier_Casado/SARIMAX/dmax.csv', mode = 'a', header = False)\n",
    "    dfdks.to_csv('/home/javier/REPOS-GH/TFM-Javier_Casado/SARIMAX/dkiss.csv', mode = 'a', header = False)\n",
    "    dfprmnt.to_csv('/home/javier/REPOS-GH/TFM-Javier_Casado/SARIMAX/paramount.csv', mode = 'a', header = False)\n",
    "    dftn.to_csv('/home/javier/REPOS-GH/TFM-Javier_Casado/SARIMAX/ten.csv', mode = 'a', header = False)\n",
    "    dftrc.to_csv('/home/javier/REPOS-GH/TFM-Javier_Casado/SARIMAX/trece.csv', mode = 'a', header = False)\n",
    "    \n",
    "    df1.to_csv('/home/javier/REPOS-GH/TFM-Javier_Casado/ML_ALGOR/df_ml.csv', mode = 'a' , header = False)\n",
    "    counter += 1\n",
    "    print('ok', counter)\n",
    "    \n",
    "    if counter == len(archivos):\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
